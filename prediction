#Importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from lightgbm import LGBMClassifier #!pip install lightgbm
from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split
from sklearn.metrics import confusion_matrix,roc_curve, accuracy_score
from sklearn import preprocessing
import warnings
warnings.filterwarnings("ignore",category=DeprecationWarning)
warnings.filterwarnings("ignore",category=FutureWarning)
df = pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv')
df.head()
df.drop(['id','Unnamed: 32'],axis = 1,inplace = True)
df.shape
df.info()
df.describe()
le = preprocessing.LabelEncoder()
df['diagnosis'] = le.fit_transform(df['diagnosis'])
y=df["diagnosis"]
X=df.drop(["diagnosis"],axis=1)
B, M = y.value_counts() #B -> 0 , M->1
print('Number of Benign',B)
print('Number of Malignant',M)
sns.countplot(df['diagnosis']);
corr_mat=df.corr()
plt.figure(figsize=(20,15))
sns_plot=sns.heatmap(data=corr_mat, annot=True)
plt.show()
sns.jointplot(X.loc[:,'perimeter_mean'], X.loc[:,'radius_mean'], kind="reg", color="#ce1414")
sns.set(style="white")
df = X.loc[:,['radius_worst','perimeter_worst','area_worst','perimeter_mean','radius_mean',]]
g = sns.PairGrid(df, diag_sharey=False)
g.map_lower(sns.kdeplot, cmap="RdBu")
g.map_upper(plt.scatter)
g.map_diag(sns.kdeplot, lw=3);
#Train test split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.30,random_state=42)
#1. Predicting with Logistic Regression
log_model = LogisticRegression(solver="liblinear").fit(X_train,y_train)
log_model.intercept_
log_model.coef_
y_pred = log_model.predict(X_test)
y_pred[:10]
log_model_matrixs = confusion_matrix(y_test,y_pred)
log_model_score = accuracy_score(y_test,y_pred)
cross_val_score(log_model,X_test,y_test,cv=10).mean()
#2. Predicting with Nearest Neighbors
knn_model = KNeighborsClassifier().fit(X_train,y_train)
y_pred=knn_model.predict(X_test)
knn_model_matrixs = confusion_matrix(y_test,y_pred)
knn_model_score = accuracy_score(y_test,y_pred)
#Model Tuning
knn = KNeighborsClassifier()
knn_params = {"n_neighbors":np.arange(1,50),"weights" : ["uniform", "distance"]}
knn_cv_model =GridSearchCV(knn,knn_params,cv=10).fit(X_train,y_train)
knn_cv_model.best_score_
knn_cv_model.best_params_
knn_tuned = KNeighborsClassifier(n_neighbors=6,weights = 'distance').fit(X_train,y_train)
y_pred = knn_tuned.predict(X_test)
accuracy_score(y_test,y_pred)
knn_tuned.score(X_test,y_test)
3.Predicting with Support Vector Machines(SVM)
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.30,random_state=42)
svm_model = SVC(kernel="linear").fit(X_train,y_train)
y_pred = svm_model.predict(X_test)
svm_model_score = accuracy_score(y_test,y_pred)
#SVM Model Tuning
svm = SVC()
svm_params = {"C":np.arange(1,10),"kernel":["linear","rbf"]}
svm_cv_model = GridSearchCV(svm,svm_params,cv=5,n_jobs=-1,verbose=2).fit(X_train,y_train)
svm_cv_model.best_params_
svm_tuned = SVC(C=7,kernel="linear").fit(X_train,y_train)
y_pred = svm_model.predict(X_test)
accuracy_score(y_test,y_pred)
#4.Predicting with Decision Trees
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.30,random_state=42)
cart_model = DecisionTreeClassifier().fit(X_train,y_train)
y_pred = cart_model.predict(X_test)
accuracy_score(y_test,y_pred)
#Model Tuning
cart = DecisionTreeClassifier()
cart_params = {"max_depth":[1,3,5,8,10],
              "min_samples_split":[2,3,5,10,20]}
cart_cv_model = GridSearchCV(cart,cart_params,cv=10,n_jobs=-1,verbose=2).fit(X_train,y_train)
cart_cv_model.best_params_
cart_tuned = DecisionTreeClassifier(max_depth=5,min_samples_split=20).fit(X_train,y_train)
y_pred = cart_tuned.predict(X_test)
accuracy_score(y_test,y_pred)
#5.Predicting with Random Forests
rf_model = RandomForestClassifier().fit(X_train,y_train)
y_pred = rf_model.predict(X_test)
accuracy_score(y_test,y_pred)
#Model Tuning
rf = RandomForestClassifier()
rf_params = {"n_estimators":[100,500,1000],
            "max_features":[3,5,8],
            "min_samples_split":[2,5,10]}
rf_cv_model = GridSearchCV(rf,rf_params,cv=10,n_jobs=-1,verbose=2).fit(X_train,y_train)
rf_cv_model.best_params_
rf_tuned = RandomForestClassifier(n_estimators=100,
                                  max_features=5,min_samples_split=2).fit(X_train,y_train)
y_pred = rf_tuned.predict(X_test)
accuracy_score(y_test,y_pred)
Importance = pd.DataFrame({"Importance":rf_tuned.feature_importances_*100},index=X_train.columns)
Importance.sort_values(by="Importance",axis=0,ascending=True).plot(kind="barh");
plt.xlabel("Variable importance")
plt.gca().legend_=None
#6.Predicting with Gradient Boosting Machines(GBM)
gbm_model = GradientBoostingClassifier().fit(X_train,y_train)
y_pred = gbm_model.predict(X_test)
accuracy_score(y_test,y_pred)
#Model Tuning
gbm = GradientBoostingClassifier()
gbm_params = {"learning_rate":[0.1,0.01,0.001,0.05],
             "n_estimators":[100,500,1000],
             "max_depth":[2,5,8]}
gbm_cv_model = GridSearchCV(gbm,gbm_params,cv=10,n_jobs=-1,verbose=2).fit(X_train,y_train)
gbm_cv_model.best_params_
gbm_tuned=GradientBoostingClassifier(learning_rate=0.1,
                                     n_estimators=1000,max_depth=2).fit(X_train,y_train)
y_pred = gbm_tuned.predict(X_test)
accuracy_score(y_test,y_pred)
#8.Predicting with LightGBM
lgbm_model = LGBMClassifier().fit(X_train,y_train)
y_pred = lgbm_model.predict(X_test)
accuracy_score(y_test,y_pred)
#Model Tuning
lgbm = LGBMClassifier()
lgbm_params = {"learning_rate":[0.1,0.01,0.001],
              "n_estimators":[100,200,500],
              "max_depth":[1,2,35,8]}
lgbm_cv_model = GridSearchCV(lgbm,lgbm_params,cv=10,n_jobs=-1,verbose=2).fit(X_train,y_train)
lgbm_cv_model.best_params_
lgbm_tuned = LGBMClassifier(learning_rate= 0.1, max_depth= 1, n_estimators= 500).fit(X_train,y_train)
y_pred = lgbm_tuned.predict(X_test)
accuracy_score(y_test,y_pred)
models = [knn_tuned,
         log_model,
         svm_tuned,
         cart_tuned,
         rf_tuned,
         gbm_tuned,
         lgbm_tuned]
        

result=[]
results = pd.DataFrame(columns=["Models","Accuracy"])

for model in models:
    names =model.__class__.__name__
    y_pred=model.predict(X_test)
    accuracy=accuracy_score(y_test,y_pred)
    result=pd.DataFrame([[names,accuracy*100]],columns=["Models","Accuracy"])
    results = results.append(result)
sns.barplot(x="Accuracy",y="Models",data=results)
plt.xlabel("Accuracy %")
plt.title("Accuracy Rate of Models")
results.sort_values(by='Accuracy',axis=0, ascending=False)
